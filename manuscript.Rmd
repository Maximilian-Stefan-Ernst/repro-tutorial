---
title: "A Hitchhiker's Guide to Reproducible Research" # replace with something serious
output: bookdown::pdf_document2
repro:
  packages:
    - usethis
    - git2r
    - aaronpeikert/repro@7bfaf98
    - here
    - rstudio/webshot2@f62e743
    - targets
  scripts:
    - R/lavaan_helper.R
    - R/simulation.R
    - R/link.R
csl: apa7.csl
numbered_headings: true
bibliography: ["references.bib", "packages.bib"]
---

<!-- the HTML comments, like this one, are meta comments, mainly describing the intent --->
<!-- each sentence below a heading summarizes what I want to say there --->
<!-- "Hands-on:" means concrete practical application, they roughly proceed from easy/familiar to hard/unfamiliar-->

```{r setup, include=FALSE}
library(repro)
library(tidyverse)
automate_load_scripts()
source(here::here("R", "link.R"))
```

# Why you should care about reproducibility

<!-- define reproducibility -->
<!-- is a direct quote from my master thesis -->
<!-- I wouldn't want to cite it. @brandmaier, would you please rewrite? -->
A reproducible scientific product allows other researchers to obtain the same results from the same dataset in a way that enables substantive criticism and therefore facilitates replication.
<!-- lure the reader in -->
The research community has long accepted reproducibility as part of "good scientific practice", but has not has not benefited much from this self-imposed requirement.
<!-- expand: who is calling it a requirement, maybe metascientific arguments-->
Reproducing an empirical study is often too tedious and frustrating work for the researchers to benefit them.
However, if reproduction would require mere minutes instead of hours, it could greatly facilitate collaboration within and across research projects.
<!-- Guiding principle: spending machine compute time instead of human research time -->
To archive this boost, we must place the burden of reproducing something upon computers instead of human researchers.
A scientific document should therefore be understood by researchers but reproduced by computers.

This tutorial walks through the creation of such a document that can be reproduced automatically.
Though reproducibility is but a small part of the research process, it can serve as the mortar of open science building blocks (e.g. preregistration, open data, postpublication review).
Therefore, we show the whole lifecycle of an open research project, where some parts are essential to archive reproducibility.
All code chunks are meant for the reader to try out for themselves.
Still, the reader can safely skip sections marked as optional, when unfit for their research question or methods.
However, any empirical data analysis will suffer from the following threats to reproducibility:
<!-- Which problems are solved by which tool (like in Andreas talk)-->
<!--stolen from: https://brandmaier.github.io/reproducible-data-analysis-materials/KULeuvenQuantPsy2020.html#11 -->

1. Multiple versions of scripts/data (e.g., the dataset has changed over time, i.e., was further cleaned or extended)
2. Copy&paste errors (e.g., inconsistency between reported result and reproduced result)
3. unclear which scripts should be executed in which order
4. Broken software dependencies (e.g., analysis broken after an update, missing package)

It is common to rely on the craftiness of the researcher to debug these problems, but we could prevent them altogether with:

1. Version control
2. Dynamic document creation
3. Dependency tracking
4. Software management

We deem each concept necessary to archive longterm reproducibility.
However, which tools you use to implement these concepts is a matter of taste and project requirements.
This tutorial follows the recommendations of @Peikert2019 and utilize Git for version control, R Markdown for dynamic document creation, Make for dependency tracking, and Docker for software management.
Their interplay is shown in Figure \@ref(fig:schematic).
At several points, we suggest other viable alternatives.

```{r schematic, eval = TRUE, echo = FALSE, fig.cap="Schematic illustration of the interplay of the four components (in dashed boxes) central to the reproducible workflow: version control (Git), dependency management (Make), software management (Docker), and dynamic document generation (R Markdown). Git tracks changes to the project over time. Make manages dependencies among the files. Docker provides a container in which the final report is built using dynamic document generation in R Markdown. Reproduced from @Peikert2019.  "}
# file gets downloaded in Makefile
knitr::include_graphics("images/nutshell.svg", auto_pdf = TRUE)
```

# Setup

<!-- keep it brief, let `repro` do the work -->

We assume that you have already installed R and RStudio (if not check the `r link("https://github.com/aaronpeikert/repro-tutorial/blob/master/install.md", "installation guide")`).
Additionally, you'll need the [`repro`-package](https://github.com/aaronpeikert/repro)[^repropackage]:

[^repropackage]: https://github.com/aaronpeikert/repro

```r
# -- type this on the R console --
if(!requireNamespace("remotes"))install.packages("remotes")
remotes::install_github("aaronpeikert/repro")
library("repro")
```

The [`repro`-package](https://github.com/aaronpeikert/repro) contains several helpers to work with reproducibility tools.
For example, it can check if your computer is set up to use the tools we rely on:

```{r}
# -- type this on the R console --
check_git()
```

```{r}
# -- type this on the R console --
check_make()
```

```{r}
# -- type this on the R console --
check_docker()
```

If these commands detect that something is not installed or set up, they will give you detailed instructions (tailored to your operating system) to remedy the situation.
Follow the instructions, then rerun the command, until it tells you not to worry.

# Planing

<!-- researcher should begin early to enjoy most benefits -->
Reproducibility is invaluable for effective collaboration---including with your "past self".
While you might be tempted to consider reproducibility only when you about to finish a project, upfront investments pay of through your increased productivity.
Therefore, we suggest starting early, ideally from the moment you decide to pursue a project to incorporate tools to help you archive reproducibility gradually.
The first step is to bundle all required files in one folder.
This way, you never lose anything, and you can easily share the files.
<!-- Hands-on: Introducing RProjects-->
RStudio's RProjects facilitate this concept of a project folder.
To create one, click on:

> File → New Project... → New Directory → New Project

## Markdown

<!--Keep notes.-->
<!-- Hands on: Introduce Markdown -->
A great tool to capture ideas or meeting notes is Markdown.
To create a Markdown document click on: 

> File → New File → Markdown File

Markdown is simply text, where some characters let you add a minimal amount of structure:

```{r, results='asis', echo=FALSE}
# in case we want to reuse this example later
markdown_example <- "
<!--this is a Markdown file -->
# Header

Normal text.
Important **word**.

To do list:

* do research
* do more research
* spend time with family
"
cat("``` markdown", markdown_example, "```", sep = "")
```

We will later see how this simplicity enables us to create many different document formats (i.e. Word, PDF, HTML) and incorporate code (i.e. R, Python, Julia).

## Git

Another advantage of Markdown is that you can easily version-control it with Git.
You may be familiar with Microsoft Words' "Track Changes"-Feature and version control is similar in its basic idea but more potent in its features.
Instead of tracking only one file, you can keep track of the whole project directory, and instead of losing all changes when you agree to them, the entire history is preserved transparently.
For example, this paper has been going through `r length(git2r::commits())` `r link("https://github.com/aaronpeikert/repro-tutorial/commits/master", "iterations")`.
To activate Git in a given project, you can call:
<!-- Hands-on: Introduce Git -->

```r
# `package::function()` → use function from package without `library(package)`
usethis::use_git()
```

Recording each change you made helps you to iterate more quickly because you know that you can effortlessly go back to previous versions.
For example, you can write down a rough project idea, knowing your collaborators and you will iterate and improve.
<!-- This is maybe the place to introduce the example-->
First, you could create a new Markdown file named `idea.md` that looks like this:

```markdown
<!--this is a Markdown file -->

# Hypothesis

Machiavellianism is higher in male persons.

# Analytic Strategy

t-test

# Sample

Could we use openpsychometrics.org data?
```

And you could commit it in Git:
<!--I am leaning towards doing everything in R, not sure though-->
<!--For Git we maybe should use RStudio's Git pane?-->

```r
git2r::add(".", "idea.md")
git2r::commit(".", "add a first concept")
```

But then you realize that the new literature suggests that there could be a bias in measurement:

``` markdown
<!--this is a Markdown file -->

# Hypothesis

Machiavellianism is higher in male persons.

# Analytic Strategy

Multigroup CFA + Measurement Invariance

# Sample

Could we use openpsychometrics.org data?
```

Commit this version:

```r
# -- type this on the R console --
git2r::add(".", "idea.md")
git2r::commit(".", "exclude bias in measurement as possible confounder")
```

And GitHub can show you what has changed, like in Figure \@ref(fig:idea-change).
GitHub is in its essence a webinterface to Git and you may learn more about how to use it under the section on [GitHub].

```{r idea-change-screenshot, include=FALSE}
# unfortunately css selectors won't work for pdf so we'll use png instead
idea_change <- here::here("images", "idea-change.png")
if(!file.exists(idea_change)){
  webshot2::webshot("https://github.com/aaronpeikert/repro-tutorial/commit/5b3f4641ff542158184f85d458880c35e8f09f3c?diff=split",
                  file = idea_change,
                  selector = "div#diff-2f8beaa39e5c98706d8abb3361a8383776f8df8a195a21c0e6eeb25e5aa48f45", # only select file change
                  zoom = 2)# higher resolution
}
```

```{r idea-change, echo=FALSE, fig.cap="A screenshot of how a change tracked in Git is represented by GitHub.  ", out.width='100%'}
knitr::include_graphics(idea_change)
```

<!--I am troubled by how we should recommend to learn git-->
Git can be challenging to learn, because we are used to learn alone and Git is best learned with others.
We recommend to find a partner to attend an online lecture or workshop to learn the ins and outs of Git.

## Simulation (optional)

<!-- impress with neatness → sample size planning, preregistration and analysis in one 😎-->
<!-- but explain that good science is often not as neat, strive for the ideal -->

```{r, include=FALSE}
source(here::here("R", "simulation.R"))
```

```{r, include=FALSE}
loadings_jones_paulhus <- c(38, 31, 40, 52, 59, 71, 62, 46, 51)/100
cohend_jones_paulhus <- c(24, 29, 35)/100
```

There are probably few things as frustrating as realizing that the data that you gathered cannot answer the research question you had in mind.
A step to prevent unpleasant surprises is to simulate data.
Simulated data lets you verify that the data you expect to gather and the analysis you plan fit together.
No matter which analytic strategy you plan, it comes with assumptions and a simulation forces you to address these assumptions more directly, than simply thinking about it.
If you identify unrealistic assumptions you can either violate these assumptions and see if the analytic strategy is robust or modify the analytical strategy to address them.
We will show an example using structural equation modeling (SEM) for our Machiavellism example.
However, the general process is applicable to any statistical model, but of course the simulation here is specific to our hypothetical research question.
One has to make many educated guesses to simulate data that closely matches your expectation.
These guesses strongly depend on the study you are planning, and there cannot be a one fits all solution.
The analytic strategy (i.e. SEM) often gives you a hint about how to simulate data.

If you are unfamiliar with SEM, you may skim over the next sections.
To simulate data for SEM, one has to specify a model and then sample from a multivariate normal distribution following its implied covariance matrix.
The model parameters should express out expectations for the Machiavellism subscale of the short dark triad (SD3).
As a starting point, we can use already published information.
@Jones2013 report loadings seen in Table \@ref(tab:loadings) and report a gender difference (at the manifest level) depending on the sample between `r min(cohend_jones_paulhus)` and `r max(cohend_jones_paulhus)`.

```{r loadings, echo=FALSE}
knitr::kable(data.frame(Item = 1:9,
                        Loading = loadings_jones_paulhus),
             caption = "Factor loadings from an Exploratory Structrural Equation Model as reported in Jones \\& Paulhus (2013)")
```

It is probably better to assume more pessimistic values.
This way, we make sure that our analytic strategy is viable even in non-optimal circumstances.
Therefore, we will assume that the loadings are 30% lower than reported and that the standardized mean difference is only 0.2.
These assumption can be simply translated into [`lavaan` [@R-lavaan]](https://lavaan.ugent.be)[^lavaan] model syntax:

[^lavaan]: https://lavaan.ugent.be

```{r, results='asis', echo=FALSE}
model_truth <- combine(
  measurement(
    "MACH",
    items("x", 9),
    loadings = round(loadings_jones_paulhus * 0.7, 2)
    ),
  intercepts("MACH", list(0, 0.2))
  )
cat("``` r",
    model_truth,
    "```",
    sep = "\n")
model <- combine(measurement(
  "MACH",
  items("x", 9)),
  intercepts("MACH", list(0, "NA")))
```

From which we can simulate data:

```{r, results='hide'}
simulated <- lavaan::simulateData(
  model_truth,
  sample.nobs = c(1000, 1000),
  meanstructure = TRUE,
  standardized = TRUE
)
```

And then fit the statistical model:

```{r}
fit <-
  lavaan::cfa(
    model,
    data = simulated,
    group = "group",
    meanstructure = TRUE,
    std.lv = TRUE
  ) 
```

But wait `lavaan` warns us that this model may not be identified.
We overlooked a crucial assumption of this model, that is measurement invariance.
This assumption is also baked into our simulation of the data.
There is a good chance that this assumption is violated.
We therefore adept our analysis to plan for this contingency.
After fitting the model with strict measurement invariance, we free equality constrains first on loadings and then on intercepts based on the Lagrange Multiplier test.
The model is iteratively refit and improved till at most three loadings or three intercepts are freed or the Lagrange Multiplier test is not significant for the constrains on loadings or intercepts.
While reconsidering our assumptions, it is also unlikely that the data is normally distributed.
Not only would the actual items be Likert-like and therefore categorical, but also are the in @Jones2013 reported means not in the middle of the scale, thus suggesting some skewness to the items.
@Rhemtulla2012 conclude that using Likert-like items is acceptable when they employ five or more categories.
For simplicity we do not address these concerns (but doing so would certainly improve the simulation).
Anyhow, remember that what we consider realistic for our case could be unfit for your case.
If you are satisfied with your assumptions it helps to write a function that expresses them, like this one:
<!--Hands-on: build simple functions tailored for your analysis-->

```{r}
generate_data
```

Note that this function is again composed of several functions, which you may inspect on `r link("https://github.com/aaronpeikert/repro-tutorial/blob/master/R/simulation.R", "GitHub")`.

With its help we can simulate 500 cases and fit our analysis.

```{r}
true_diff <- 0.2
model_truth <- combine(
  measurement(
    "MACH",
    items("x", 9),
    loadings = round(loadings_jones_paulhus * 0.7, 2)
  ),
  intercepts("MACH", list(0, true_diff)),
  variances("MACH", list(1, 1))
)

model_same <- 
  "MACH =~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9
  MACH ~ c(0,0)*1
  MACH ~~ c(1, NA)*MACH"

model_differ <- 
  "MACH =~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9
  MACH ~ c(0, NA)*1 + c(zero, diff)*1
  MACH ~~ c(1, NA)*MACH"
generate_data(500,
              model_truth,
              model_same,
              model_differ,
              list(std_estimate = get_std_estimate),
              group.equal = c("loadings", "intercepts"),
              mod_load = 3,
              mod_int = 3,
              std.lv = TRUE)
```

Note, that this certainly results in different numbers each time we run it.

```{r}
generate_data(500,
              model_truth,
              model_same,
              model_differ,
              list(std_estimate = get_std_estimate),
              group.equal = c("loadings", "intercepts"),
              mod_load = 3,
              mod_int = 3,
              std.lv = TRUE)
```

To consistently get the same results it is good practice to use a seed with `set.seed(1235)`.
It is a little more complicated when you have parallelized your code, like we did, but the future package handles this for us:

```{r}
set.seed(1235)
runif(1)
set.seed(1235)
runif(1)
```

```{r}
future::value(future::future(runif(1), seed = 1235))
future::value(future::future(runif(1), seed = 1235))
```

### Preregestration (optional)

<!--Hands-on: build simple functions tailored for your analysis-->

A preregistration serves as a tool that may help to increase the credibility of empirical results.
Without getting lost in the meta scientific arguments for preregistration, we want to note two advantages.
First, a preregistration is an opportunity to carefully plan a study.
Gathering empirical data often requires enormous resources and better planning increases the chances that these efforts are worthwhile.
For us a good preregistration is the same as a good plan; the only difference is that a preregistration is made public beforehand.

The second advantage is transparency.
A preregistration offers the interested reader more insight into the research process than a paper alone could.
Some [citation!] argue that preregistration can not fulfill its promise to reduce the false discovery rate of science, if we allow to deviate from them.
While such argument has some merits, we believe that transparency outweighs these concerns.
If researchers deviate from their preregistration, they probably have good reason to do so, which means they have learned something from the process of conducting the study.
A preregistration, together with version control allows their readers to inform themselves about these lessons learned.

The liklihood that you have to diverge from your preregistration increases the more specific and concrete the preregistration is.
To nudge researcher to be specific enough, there are many preregistrations template.
These templates formalize somewhat the research questions and the planned analysis.
An alternative to such protocol is to submit preregistration as code.
Therefore, instead of describing the analysis that you plan to turn into code, you skip a step and write the code beforehand.
This is obviously challenging to do, because it leaves no place to hide in the additional layer of abstraction.
It may well be that for any given project this is impossible, without deviating from the preregistration.
However, preregistered and version controlled code lets anyone see what was changed very concretely.

One way to ensure that the code for the analysis works (at least in theory) is to simulate data.
A data simulation allows you to verify that the code runs error free and lets you form more realistic expectation about the correctness and precision of the estimates.
We, for example, simulated data for the Machiavellism subscale of the short dark triad (SD3) and analyzed this with structural equation modelling (SEM).
This showed that we have to gather at least `missing_rcode_simulation` observations per group to archive enough precision and that our solution to a data problem we expect (insufficient measurement invariance) introduces a bias of `missing_rcode_simulation`%.
If you went to the trouble to simulate data, we argue that you get a preregistration for free.

### R Markdown

<!--Hands on: Introduce R Markdown-->
A preregistration in code of course describes sufficiently how you want to analyze the data, but ideally you also describe the why.
Literate programming combines code and text, enabling you to explain your reasoning.
As @knuthCWEBSystemStructured states: "The main idea is to regard a program as a communication to human beings rather than as a set of instructions to a computer."
We already learned about [Markdown], to write text.
R Markdown allows you to include R code (in fact arbitrary computer code; see @riedererChapter15Other).
To create a R Markdown document click on: 

> File → New File → Markdown File

The code is separated from the text by three backtics, like here:

````md
```{r}`r ''`
set.seed(1235)
runif(1)
```
````

When you include code into a R Markdown this code is run and its results are included in the rendered document.
With R Markdown, you can write whole articles (like this one).
You could in principle even write large parts of the manuscript as part of your preregistration, i.e. the introduction, theoretical background, the method section and parts of the discussion.
In fact, if you have simulated data, you could even include fake results.
Such dynamic document is arguably the most concrete preregistration possible.
One advantage is that writing the manuscript and writing the preregistration are no longer two distinct tasks.
Therefore a preregistration can be thought of as simply rearranging tasks that you have to do anyway.

<!--Hands on: gitignore resulting .html/pdf-->
<!-- A preregistration needs to be public -->

## Prepare public release (optional)

One of the greatest benefits of open research is that other people can freely reuse materials.
To use your materials, they first have to find them and learn how to use them.
Interested users find out about your project via the readme.
The readme is the first thing they see when they visit your project on [GitHub].
It should explain to them what your project is about and how to reproduce your project.
You could for example include the abstract of your (planned) paper here.

To add a readme to a repository run:

<!--Hands on: Add a readme-->
```r
usethis::use_readme_rmd()
```

This command will create a file with the name `README.Rmd`.
<!--knit/rmarkdown will be explained in section above-->
When you knit this file, it in turn creates a `README.md`.
That means you can include R code or plots etc. within the readme.
The `README.md` file will look pleasant in the browser, just like in Figure \@ref(fig:readme).

```{r readme-screenshot, include=FALSE}
# unfortunately css selectors won't work for pdf so we'll use png instead
readme <- here::here("images", "readme.png")
webshot2::webshot("https://github.com/aaronpeikert/repro-tutorial/",
                  file = readme,
                  vwidth = 1920, # height & width are not resolution
                  vheight = 1080, # but what website thinks the browser size is
                  selector = "div#readme",
                  zoom = 2) # higher resolution
```

```{r readme, echo=FALSE, fig.cap="A screenshot of how a README.md is rendered/presented on GitHub.  ", out.width='100%'}
knitr::include_graphics(readme)
```

Even when other researchers are excited to use your code or materials, they are usually legally inhibited from to doing so.
That is because in many countries (i.e. the USA, European Union) the creator of something has exclusive right to use their creation.
A license allows other people to use what you have created.
<!---exact copy from our previous manuscript, do we need to change? -->
In our experience, the [Creative Commons - Attribution license (CC-BY)](https://creativecommons.org/licenses/by/4.0/) is often appropriate for sharing texts, R Markdown files, generated figures, and other media, whereas scripts and any other computer code are often best shared under the [MIT license](https://opensource.org/licenses/MIT) (or similar permissive licenses).
Both licenses assure maximal freedom for future users while requiring the attribution of the original authors in derivative work.
These licenses are also in line with the recommendations by the Reproducible Research Standard [@stoddenEnhancingReproducibilityComputational2016; @stoddenEnablingReproducibleResearch2009].
A great resource to choose a license is [choosealicense.com](https://choosealicense.com), however, no resource, including our recommendation, replaces legal advice.
<!-- plagiary end -->
It is important to get written consent from collaborators when you change/add the license.
However, if their first contribution occurs after you have added a license, they give their consent by contributing.
To add a CC-BY license to your project simply run (there are similar commands for other licenses):

<!--Hands on: Add a license-->
```r
usethis::use_ccby_license()
```

We strongly suggest that you strive for respectful interactions with anyone who might want to contribute.
In fact, we hope it comes naturally to you.
To communicate that contributors can expect a welcoming and respectful environment, you can add the `r link( "https://www.contributor-covenant.org", "Contributor Covenant")` to your repository.
Contributing to an open project is daunting and a public pledge to value any contributions (even when you do not incorporate them) can help others to take the first step.
The Contributor Covenant is a code of conduct used by many open source project.

To add the Contributor Covenant to your project use:

<!--Hands on: Add a code of conduct-->
```r
usethis::use_code_of_conduct()
```

However, there is some controversy around how to handle violations of a code of conduct.
For that or any other reason, you may want to adapt the Contributor Covenant, to fit the needs of you project (which you are allowed to do because it is published with a CC-BY license).
You can also consider alternative Codes of Conduct, like the `r link("https://opensource.microsoft.com/codeofconduct/", "Microsoft Open Source Code of Conduct")` or the `r link("https://berlincodeofconduct.org", "Berlin Code of Conduct")`.

If you have considered what you want to tell the public about the project with a readme, have chosen a license and thought about how to make collaboration welcoming, you may publish your Git repository.
See the section on [GitHub] on how to publish your project.

## Collaboration

Often, reproducibility is reduced to the merits it has for the reader.
However, we argue that reproducibility is vital for collaboration, not only in the sense that the consumer of a scientific product build upon it, but also that the creators work together.
The time that you invest into ensuring reproducibility, is directed at ensuring effective collaboration.
Therefore, reproducibility is at its core about collaboration.

### GitHub

On your local machine, you use Git to track changes.
Online services like GitHub allow you to share these changes with others.
GitHub is not the only solution; there is also [GitLab](gitlab.com) or [BitBucket](https://bitbucket.org), which are as feature-rich as GitHub.
GitLab has the advantage that your institution can host it themselves; therefore, you are not relying on commercial service providers.
Anyhow, we focus on GitHub because it is the most popular service.
To upload your changes to GitHub, you need to create an account under https://github.com/join.
As a researcher, you are eligible for the `r link("https://help.github.com/en/articles/applying-for-an-educator-or-researcher-discount", "Researcher/Educator discount")`.
The following command creates a new GitHub repository under your GitHub account (beware this is public):
<!--Hands-on: Introduce Github-->

```r
usethis::use_github()
```

To create a private repository (only you and people you invite have access), use:

```r
usethis::use_github(private = TRUE)
```

Only from this moment on, files are uploaded to the internet and only those which you have explicitly added and committed in Git.

<!--Hands on: Introduce GitHub Issues-->
<!--Hands on: Introduce GitHub Projects-->

# Analyzing Data

While you already know how to write dynamic documents with [Markdown] and [R Markdown], and track changes with [Git] and [GitHub], two components to archive longterm reproducibility are still missing, namely internal and external dependencies.

## Internal Dependencies

<!--I am not sure this is the right place, should we introduce this earlier? -->

A reader of your reproducible manuscript should be able to derive how they are to reproduce it (ideally by a statement in your [README]).
To be able to do that, it is often key to understand how different components of the project relate to each other.
You cannot run an analysis without data, you cannot render a dynamic document without an analysis, and so forth.
Dependency management means that you capture the interrelations between the components of your analysis.
The "components" of an analysis are often loosely mapped onto files.
For a simple project it might be enough to have a single R Markdown.
However, while it is easy to understand how to reproduce the end result, this approach quickly gets unwieldy.
To better navigate the data analysis, a researcher often splits it across several scripts.
For example, they create a script to load the data and another to run the analysis.
This modularisation facilitates reuse, because the loading script may be used by other projects building on the same data, while the analysis script may be employed on other data and so forth.
The downside is that reproducibility is complicated, because instead of reproducing everything in one step, multiple steps are required.

Of course there a several ways to solve this problem.
One could build an R package to separate analysis and data or use code externalization in the RMarkdown to facilitating reuse or build a "master" script that combines all steps.
We found Make to be particularly useful for two reasons.
First, it is language agnostic, therefore fusing steps that are carried out in different programming languages.
Second, it does implement a simple, yet powerful caching mechanism of results which is welcome in computing intensive analysis often found in neuroimaging or machine learning.

Make is build around the concept of computational recipes.
Like cooking recipes, the dish you want to create is at the top, followed by the necessary ingredients with the procedure at last.
In Make the dish is usually a file and is called "target".
The ingredients are called "requirements" and are usually also files.
The procedures are formulated as shell commands.
One (hypothetical) example is this:

```
spaghetti_arrabiata.pdf: spaghetti_arrabiata.Rmd arrabiata_sauce.csv pasta.csv
  Rscript -e 'rmarkdown::render("spagetti_arrabiata.Rmd")'
```

An important property is that each requirement can be itself be a target, like here:

```
spaghetti_arrabiata.pdf: spagetti_arrabiata.Rmd arrabiata_sauce.csv pasta.csv
  Rscript -e 'rmarkdown::render("spaghetti_arrabiata.Rmd")'

pasta.csv: cook_pasta.R
  Rscript -e 'source("cook_pasta.R")'

arrabiata_sauce.csv: cook_sauce.R canned_tomatoes.csv
  Rscript -e 'source("cook_sauce.R")'
```

Lets suppose you want to eat spaghetti arrabiata and you still have delicious arrabiata sauce in the fridge.
Would you cook the sauce again?
No, you would only cook the pasta.
Make behaves the same.
If a file already exists it does not recreate it unnecessarily.
However, if a requirement is newer than its target, make recreates the target to reflect the updated dependency.
Therefore, when you update the data file, the analysis is run again.
Because Make knows the whole dependency tree, its caching is smarter than many alternatives.

One alternative that is even smarter than Make is the R package [`targets` [@targets2021]](https://books.ropensci.org/targets/)[^targets].
`targets` can infer more about your project than Make, because it analysis the R code you write to build the dependency tree.
For this inference to work well you have to write everything in R and you have to write everything strictly functional.

[^targets]: https://books.ropensci.org/targets/

<!--Hands on: Introduce Make-->
<!--Hands on: Add targets for README.Rmd, preregistration.Rmd -->

The `repro` package can automatically write Makefiles for simple scenarios.
For this purpose, `repro` utilizes the YAML metadata:

```{r}
x <- readLines(here::here("preregistration.Rmd"))
yaml_ind <- stringr::str_which(x, "^---[[:space:]]*")
stripped <- stringr::str_c(x[seq(yaml_ind[[1]], yaml_ind[[2]])],
                  collapse = "\n")
cat("```\n", stripped, "\n```", sep = "")
```


``` r
automate_make()
```

## External Dependencies

A considerable number of external dependencies may hinder collaboration and long term reproducibility, but automated solutions can "bundle" and fix dependencies.

<!-- Hands on: Introduce Docker -->

``` r
automate_docker()
```

<!-- after this everything is optional? -->

### renv (alternative)

### Singularity (optional)
<!-- Hands on: add data via automate() -->
<!-- Hands on: automatically download data via manually editing the Makefile -->

## Data Integrity (optional)

<!-- Hands-on: check changed hash -->

## Anonymising Data (optional)

<!-- Should we add a section on synthetic data generation? -->
<!-- Hands on: Create synthetic data -->

## Releasing a Preprint (optional)

<!-- Hands on: add commit hash to results-->
<!-- Hands on: git tag + github release-->

# Peer Review (optional)

# Post Publication Review (optional)

# Archiving

<!-- Hands on: explain how to archive data-->
<!-- Hands on: explain how to archive docker images-->

# Checklist

<!-- Should we design a checklist? -->

```{r, results='asis', echo=FALSE}
link_index()
```

# References

```{r, include=FALSE}
knitr::write_bib(c(.packages(), "bookdown", "lavaan", "knitr", "targets"), "packages.bib")
```

