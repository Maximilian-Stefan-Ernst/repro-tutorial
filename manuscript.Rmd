---
title: "A Hitchhiker's Guide to Reproducible Research" # replace with something serious
output: pdf_document
repro:
  packages:
    - usethis
    - repro
    - here
---

<<!-- the HTML comments, like this one, are meta comments, mainly describing the intent --->
<!-- each sentence below a heading summarizes what I want to say there --->
<!-- "Hands-on:" means concrete practical application, they roughly proceed from easy/familiar to hard/unfamiliar-->

```{r setup, include=FALSE}
library(repro)
source(here::here("R", "link.R"))
```

# Why you should care about reproducibility

<!-- define reproducibility -->
<!-- is a direct quote from my master thesis -->
<!-- I wouldn't want to cite it. @brandmaier, would you please rewrite? -->
A reproducible scientific product allows other researchers to obtain the same results from the same dataset in a way that enables substantive criticism and therefore facilitates replication.
<!-- lure the reader in -->
This notion of independent verifiability has long been part of "good scientific practice", but has not reached its full potential to benefit the scientific community.
<!-- expand: who is calling it a requirement, maybe metascientific arguments-->
Reproducing an empirical study is often tedious and frustrating work for the researcher.
However, if reproduction would require mere minutes instead of hours, it could greatly facilitate collaboration within and across research projects.
<!-- Guiding principle: spending machine compute time instead of human research time -->
To archive this boost, we must place the burden of reproducing something upon computers instead of human researchers.
A scientific document should therefore be understood by researchers but reproduced by computers.

This tutorial walks through the creation of such a document that can be reproduced automatically.
Though reproducibility is but a small part of the research process, it can serve as the mortar of open science building blocks (e.g. preregistration, open data, postpublication review).
Therefore, we show the whole lifecycle of an open research project, where some parts are essential to archive reproducibility.
Still, the reader can safely skip sections marked as optional, when unfit for their research question or methods.
However, any empirical data analysis will suffer from the following threats to reproducibility:
<!-- Which problems are solved by which tool (like in Andreas talk)-->
<!--stolen from: https://brandmaier.github.io/reproducible-data-analysis-materials/KULeuvenQuantPsy2020.html#11 -->

1. Multiple versions of scripts/data (e.g., dataset has changed over time, i.e., was further cleaned or extended)
2. Copy&paste errors (e.g., inconsistency between reported result and reproduced result)
3. unclear which scripts should be executed in which order
4. Broken software dependencies (e.g., analysis broken after update, missing package)

It is common to rely on the craftiness of the researcher to debug these problems, but we could prevent them altogether with:

1. Version control
2. Dynamic document creation
3. Dependency tracking
4. Software management

We deem each concept necessary to archive longterm reproducibility.
However, the tools you use to archive them are a mere matter of taste and project requirements.
This tutorial will follow the recommendations of @peikertMissing and utilizes Git for version control, RMarkdown for dynamic document creation, Make for dependency tracking and Docker for software management.
Their interplay is shown in Figure \@ref(fig:schematic).
At several points we suggest other viable alternatives.

```{r schematic, eval = TRUE, echo = FALSE, fig.cap="Schematic illustration of the interplay of the four components (in dashed boxes) central to the reproducible workflow: version control (git), dependency management (Make), containerization (Docker), and dynamic document generation (R Markdown). Git tracks changes to the project over time. Make manages dependencies among the files. Docker provides a container, in which the final report is built using dynamic document generation in R Markdown. Reproduced from @peikertMissing  "}
# file gets downloaded in Makefile
knitr::include_graphics("images/nutshell.svg", auto_pdf = TRUE)
```

# Setup

<!-- keep it brief, let `repro` do the work -->

We assume that you have already installed R and RStudio (if not check the `r link("https://github.com/aaronpeikert/repro-tutorial/blob/master/install.md", "installation guide")`).
Additionally, you'll need the [`repro`-package](https://github.com/aaronpeikert/repro)[^repropackage]:

[^repropackage]: https://github.com/aaronpeikert/repro

```r
if(!requireNamespace("remotes"))install.packages("remotes")
remotes::install_github("aaronpeikert/repro")
library("repro")
```

The [`repro`-package](https://github.com/aaronpeikert/repro) contains several helpers to work with reproducibility tools.
For example, it can check if you computer is set up to use the tools we rely on:

```{r}
check_git()
```

```{r}
check_make()
```

```{r}
check_docker()
```

If these commands detect that something is not installed or set up, they will give you detailed instructions (tailored to your operating system) on how to remedy the situation.
Follow the instructions, then rerun the command, till it tells you to not worry.

## Planing

<!-- researcher should begin early to enjoy most benefits -->

Organize files in one place.

<!-- Hands-on: Introducing RProjects-->

Keep notes.

<!-- Hands on: Introduce Markdown -->

Record changes.

<!-- Hands-on: Introduce Git -->

<!-- This is maybe the place to introduce the example-->
<!-- something roughly as complex as a bachelor thesis, like:

Hypothesis: Machivallianism is higher in male persons
Sample: Adults from western countries
Analytic strategy: Multigroup CFA + Partial Measurement Invariance
Data from: https://openpsychometrics.org/_rawdata/ either SD3 or MACH-IV
-->

```{r}
check_git()
```


### Simulation (optional)

<!-- impress with neatness â†’ sample size planning, preregistration and analysis in one ðŸ˜Ž-->
<!-- but explain that good science is often not as neat, strive for the ideal -->

Simulating data helps prevent unpleasant surprises and lets you double-check that the data you expect to gather and your analysis fit together.

<!--Hands-on: build simple functions tailored for your analysis-->

### Preregestration (optional)

A preregistration is a tool that may help to increase the credibility of empirical results.

<!--Hands on: Introduce RMarkdown-->
<!--Hands on: gitignore resulting .html/pdf-->
<!-- A preregistration needs to be public -->

## Prepare public release (optional)

<!--Hands on: Add a readme-->
```r
usethis::use_readme_rmd()
```

<!--Hands on: Add a license-->
```r
usethis::use_cc0_license()
```

<!--Hands on: Add a code of conduct-->
```r
usethis::use_code_of_conduct()
```

## Collaboration

### GitHub

GitHub/GitLab are great as a shared information basis: sharing and editing files, discussing and distributing tasks via issues and documenting the project via wikis even before data collection starts.

<!--Hands-on: Introduce Github-->

```r
usethis::use_github()
```

<!--Hands on: Introduce GitHub Issues-->
<!--Hands on: Introduce GitHub Projects-->

### Internal Dependencies

A well defined "entry point" that automatically reproduces everything eases collaboration.

<!--I am not sure this is the right place, should we introduce this earlier? -->

<!--Hands on: Introduce Make-->
<!--Hands on: Add targets for README.Rmd, preregistration.Rmd -->

``` r
automate_make()
```

### External Dependencies

A considerable number of external dependencies may hinder collaboration and long term reproducibility, but automated solutions can "bundle" and fix dependencies.

<!-- Hands on: Introduce Docker -->

``` r
automate_docker()
```

<-- after this everything is optional? -->

#### renv (alternative)

#### Singularity (optional)

## Analyzing Data

<!-- Hands on: add data via automate() -->
<!-- Hands on: automatically download data via manually editing the Makefile -->

### Data Integrity (optional)

<!-- Hands-on: check changed hash -->

### Anonymising Data (optional)

<!-- Should we add a section on synthetic data generation? -->
<!-- Hands on: Create synthetic data -->

## Releasing a Preprint (optional)

<!-- Hands on: add commit hash to results-->
<!-- Hands on: git tag + github release-->

## Peer Review (optional)

## Post Publication Review (optional)

## Archiving

<!-- Hands on: explain how to archive data-->
<!-- Hands on: explain how to archive docker images-->

## Checklist

<!-- Should we design a checklist? -->`
