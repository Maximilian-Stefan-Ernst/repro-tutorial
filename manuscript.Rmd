---
title: "A Hitchhiker's Guide to Reproducible Research" # replace with something serious
author: Aaron Peikert and Andreas M. Brandmaier 
output:
  bookdown::pdf_document2:
    keep_tex: TRUE
    toc: FALSE
repro:
  packages:
    - tidyverse
    - usethis
    - gert
    - aaronpeikert/repro@7bfaf98
    - here
    - rstudio/webshot2@f62e743
    - targets
    - renv
    - slider
    - patchwork
    - knitr
    - pander
    - lavaan
    - furrr
    - future.batchtools
    - rticles
  scripts:
    - R/simulation.R
    - R/simulation_funs.R
    - R/link.R
csl: apa7.csl
numbered_headings: true
bibliography: ["references.bib", "packages.bib"]
abstract: "`r tryCatch(trimws(readr::read_file('abstract.Rmd')))`"
---

<!-- the HTML comments, like this one, are meta comments, mainly describing the intent --->
<!-- each sentence below a heading summarizes what I want to say there --->
<!-- "Hands-on:" means concrete practical application, they roughly proceed from easy/familiar to hard/unfamiliar-->

```{r setup, include=FALSE}
library(repro)
automate_load_packages()
automate_load_scripts()
source(here::here("R", "link.R"))
```

```{r echo=FALSE}
# define a print method for objects of the class data.frame
knit_print.data.frame = function(x, ...) {
  res = paste(c('', '', kable(x)), collapse = '\n')
  asis_output(res)
}
# register the method
registerS3method("knit_print", "data.frame", knit_print.data.frame)
```

# Introduction

Increasingly, scientists openly share their data, materials, and analysis code.
Sharing these digital artifacts increases scientific efficiency by enabling researchers to learn from each other, reuse materials, or review published research results.
However, making a research project reproducible comes with a variety of challenges and there is a dire need for standards of reproducibility.

This paper has two aims.
First, we want to introduce the reader to a workflow [see @Peikert2019 for details] that ensures reproducibility---defined as the ability of anyone to obtain identical results from the *same* data with the *same* computer code. 
The research community has long accepted reproducibility as integral to "good scientific practice", because empirical research hinges on its objectivity meaning "in principle it can be tested and understood by anybody." [@popperLogicScientificDiscovery2002, p. 22][^popper]
Often, however, projects can not be reproduced by other research teams, despite the best efforts by the creators.
We present four common threats to reproducibility and explain how we can address these with a combination of four solutions from the software engineering community.
Specifically, we demonstrate how the R package `repro` supports researchers in implementing this solution.
The second objective is to present an alternative preregistration scheme that builds upon a reproducible workflow, the so-called preregistration as code (PAC), where the researchers write the bulk of the manuscript, including the sections Introduction, Methods, and Results---initially based on simulated data, before they gather data.

[^popper]: In fact @popperLogicScientificDiscovery2002's famous criterion of demarcation is build around "inter-subjective testing", a concept which he later generalized to inter-subjective criticism (Footnote *¹, p. 22). Somewhat confusingly @popperLogicScientificDiscovery2002 uses the term "Reproducibility" for what we would call "Replication".

This tutorial is aimed at researchers who are willing to make code, data, and materials available and should help them to ensure reproducibility of relevant statistical results.
While the willingness to share materials is required for reproducing a project, it is not sufficient.
For example, @hardwicke2018 attempted to reproduce results from open materials in the journal Cognition. 
Out of 35 published articles, results of 22 articles could be reproduced but in 11 of these cases, assistance from the original authors was required.
For 13 articles, at least one outcome could not be reproduced—--even with the original authors' assistance.
@obels2020 showed that in 62 Registered Reports, 41 had data available, and 37 had analysis scripts available.
The authors could execute only 31 of the scripts without error and were able to reproduce the results of only 21 articles (within a reasonable period of time).

Even though transparency has increased across scholarly disciplines (that is, data and methods are increasingly openly shared), most of these open repositories do not provide sufficient information to reproduce relevant computational and statistical results.
These failed attempts to reproduce highlight the need for widely accepted reproducibility standards.
To increase the proportion of reproducible projects, such reproducibility standard has to balance the trade-off between being rigorous and being easy to use.
A rigorous standard increases the likelihood of any individual project to be actually reproducible.
An easy to use standard on the other hand is more likely to be employed across many research projects.
The remainder of this paper is structured along these lines.
The first part introduces theoretical concepts and software solutions that together form a rigorous reproducibility standard.
The second part is a tutorial that shows how these software solutions can be easily employed by researchers with the help of the R package `repro`.

After introducing the conceptual building blocks, this tutorial guides the reader through each step of an open research project with an emphasis on guaranteeing reproducibility from the very first step on.
We have structured the tutorial with a _learning by doing_ approach in mind, such that readers can follow along at their own computers.
We explicitly encourage readers to try out all R commands for themselves.
Unless stated otherwise, all code blocks are meant to be run in the statistical programming language R (tested with version `r with(R.version, paste0(major, ".", minor))`).
To guide the reader through all steps of a research project, we will use a exemplary research question:

> "Is there a mean difference in the personality trait 'Machiavellism' between genders?"

This example serves only didactic purposes and not to derive substantive claims.
Nevertheless, everything else is kept as close to a real research question as possible.

# Concepts and Software Solutions

We identified the following common threat to reproducibility: <!-- CJ How did you identify these causes? Is this exhaustive?-->

1.	The existence of multiple inconsistent versions of code, data, or both; for example, the dataset could have changed over time because outliers were removed at a later stage, or the analysis code could have been modified during writing of a paper because a bug was removed at some point in time; it may then be unclear, which version of code and data was used to produce some reported set of results.
2.	Copy-and-paste; for example, often, results are manually copied from a statistical computing language into a text processor; if a given analysis is re-run and results are manually updated in the text processor, this may inadvertently lead to inconsistencies between the reported result and the reproduced result;
3.	Ambiguous order of code execution; for example, with multiple data and code files, it may be unclear which scripts should be executed in what order; 
4.	Code dependencies; for example, a given analysis may depend on a specific version of a specific software package that might not be available on a different computer or no longer exist; or, a different version of the same software may produce different results.
<!-- CJ 5. Missing steps: Some of the steps are documented (e.g., final analysis), but other steps were conducted manually and not documented (e.g., copy-pasting results from one program to another; reverse coding items on the fly).-->

We have developed a workflow that leverages established tools and practices from software engineering to achieve long-term and cross-platform computational reproducibility of scientific data analyses resting on four pillars that address the aforementioned causes of non-reproducibility [@Peikert2019]: <!-- CJ In fact, this is the central message of your paper: "We have developed this new workflow, based on four pillars." I would suggest mentioning this very clearly in the opening paragraph. The opening paragraph should at least:
1) Introduce the topic area
2) Introduce the problem/knowledge gap
3) Introduce how you will fill this knowledge gap-->

1. Version control
2. Dynamic document creation
3. Dependency tracking
4. Software management

The remainder of this section briefly explains why each of these four building blocks is needed and the role they play in ensuring reproducibility.

First, to resolve ambiguity across multiple versions of code and data that may be come into existence during the development phase of a research project, we recommend the use of a version control system.
A version control system tracks changes to all project-related files (e.g., materials, data, and code) over time.
At a later stage, single files or the entire project can be compared to or reverted to an earlier version.
Additional features, such as branches, facilitate collaboration.
Specific snapshots of a repository can be marked, such that one can clearly label a given project status as the one that created a given preregistration, a given preprint, or the final authors' version accepted at a journal.
Using version control protects against loss of files and also supports remote collaboration with semi-automated tools for tracking and merging changes.

Second, we rely on dynamic document generation.
The traditional way of writing a scientific report based on a statistical data analysis uses two separate steps which are conducted in two different programs.
In a word processor the researcher writes the text and in another software they conduct the analysis.
Both steps are linked by manually copying and pasting results from one software to the other; a process that often produces inconsistencies [@nuijtenPrevalenceStatisticalReporting2016].

Dynamic document generation fuses both steps and can be traced back to @knuthCWEBSystemStructured who put it this way: "The main idea is to regard a program as a communication to human beings rather than as a set of instructions to a computer."
This paradigm has gained popularity with the Sweave approach [@leisch2002sweave] that allowed embedding the data analysis into a TeX document, which then could be dynamically rendered into a final manuscript.
As of recently, R Markdown has added to the success of this idea.
R Markdown uses Markdown for writing the text, and R (or other programming languages) for the analysis. Markdown is a lightweight text format that is very close to plain text with a minimal subset of reserved symbols for formatting instructions.
This way, Markdown does not need any specialized software for editing, it is highly fool-proof (unlike for example LaTeX), works well with version control systems, and can be ported to various document formats, such as HTML Websites, a Microsoft Word document, a typeset PDF file (for example, via LaTeX journal templates), or a Powerpoint presentation.
We suggest to use Markdown for everything, starting from simple sketches of your ideas to your scientific manuscripts[@R-rticles] and presentations[@revealjs]---and even your CV [@vitae].
R Markdown extends regular Markdown by allowing users to include R code chunks (in fact, arbitrary computer code; see @riedererChapter15Other) into a Markdown document.
Upon rendering of the document, the code blocks are executed and their output is dynamically inserted into the document.
This allows the creation of (conditionally) formatted text, statistical results, or figures that are guaranteed to be up-to-date because they are created every time anew as the document is rendered to its output format (e.g., presentation slides or a journal article).

One would hope that sharing a literate statistical analysis would allow others (including one own's future self) to simply download and run (ie., reproduce) a given analysis.
However, as @ argue, in most instances this approach fails.
In practice, dependencies on external software (such as system libraries or components of the programming language) are frequently unmentioned and the components or packages are not included.
Troubleshooting  issues specific to a certain programming language or dependent tool typically requires considerable expertise and hinder reproducibility.

```{r, include=FALSE}
container_size <- as.numeric(fs::file_size("reprotutorial.sif"))/1024^3
```

One solution to this problem is to provide not only the analysis script but all dependent software packages and system libraries in a so-called _container_.
"By packaging the key elements of the computational  environment  needed  to  run  the  desired  software, [...] they  make the software much easier to use, and the results easier to reproduce [@silver2017].
Instead of leaving it to the user which software should be installed how on their computer, Docker provides all software used in an analysis, including the operating system itself.
The operating system level is important because some functionality may be passed to software layers beneath the programming language, such as calls to random number generators, linear algebra libraries and such.
Docker does this without interfering with the already installed software by using a virtual software environment that is independent of the host software environment.
Such a snapshot of the software stack is called an "image".
Packaging all needed software in such an image requires considerable amounts of storage space.
There are two major strategies helping to keep the storage requirements reasonable.
First, there is a community that maintains images for particular purposes.
For example, there are ready-made images for R, based on Ubuntu (a Linux operating system) containers [@boettigerIntroductionRockerDocker2017].
Users can then install whatever they need in addition to what is provided by these pre-compiled images.
`r ifelse(is.na(container_size), "", glue::glue("The image that was used for this article uses {round(container_size, 2)}Gb of disc space."))`
Our image includes Ubuntu, R, R Studio, LaTeX as well as a variety of R packages like the tidyverse [@tidyverse], amounting to `r installed.packages() %>% as.data.frame() %>% filter(is.na(Priority)) %>% nrow()` packages.
Second, instead of saving a binary image of the software, one can also save a container recipe, a so-called  `Dockerfile`.
This recipe is a textual description of all commands that need to be executed to recreate the image.
Such files are tiny (the `r link("https://github.com/aaronpeikert/repro-tutorial/blob/main/Dockerfile", "Dockerfile")` for this project is `r round(as.numeric(fs::file_size("Dockerfile"))/1024^1, 2)`Kb big) and are an unambiguous description of how to recreate a given container image.
However, they rely on the assumption that all software repositories will remain accessible and will continue to make available historic software versions.
For archiving, we therefore recommend that a full image is always kept (see Section on [Archiving Docker]).
With the `repro`-package adding R packages dependencies is as simple as adding them to the header under the tag `packages:`

The fourth and last concept we rely on is 
In software projects, tools like the make utility, are used to automatically determine, which pieces of a program need to be recompiled when the source code or dependent libraries are changed and also to issue the commands how to build the final product.
In essence, developers provide a single _Makefile_, which can be thought of as a collection of computational recipes.
Similar to a collection of cooking recipes, we can have multiple products (_targets_ in Make speak) with different ingredients (_requirements_) and different steps of preparation (_recipe_).
In the context of a scientific data analysis, the targets are typically the final scientific report (e.g., the one to be submitted to a journal) and possibly intermediate results, such as preprocessed data files, simulation results, analysis results).
To make this more concrete, consider the following hypothetical example, in which a research project contains a script to simulate data (`simulate.R`) and a scientific report of the simulation results written in R Markdown (`manuscript.Rmd`).
A Makefile could then look like this:

```
manuscript.pdf: manuscript.Rmd simulation_results.csv 
  Rscript -e 'rmarkdown::render("manuscript.Rmd")'

simulation_results.csv: simulate.R
  Rscript -e 'source("simulate.R")'
```

There are two targets, the final rendered report (`manuscript.pdf`, l.1) and the simulation results (`simulation_results.csv`, l.4).
Each target is followed by a colon and a list of requirements.
If a requirement is  newer than the target, the recipe will be executed to rebuild the target.
If a requirement does not exist, Make seeks to find a recipe to build the requirement first before building the target.
Here, if one were to build the final `manuscript.pdf` by rendering the R Markdown with the command shown in l.2, Make would make sure that the file `simulation_results.csv` exists; if not, it would find that this is also a target and issue the command in l.5 to run the simulation first before rendering the manuscript.
This way, we ensure that the simulation is always run before the manuscript is built and that the manuscript is rebuilt if the simulation code was changed. <!-- AP: I was just wondering... is this true? Is the caching mechanism transitive?-->

To summarize, the workflow by @Peikert2019 requires four components (see Figure \@ref{schematic}), dynamic document generation (using R Markdown), version control (using Git), internal dependency management (using Make), containerization (using Docker).
While R Markdown and git are well integrated into the R environment through R Studio, Make and Docker require a level of expertise that is often beyond the training of scholars outside studies of information technology, which represents a considerable hurdle in the acceptance and implementation of the workflow.
To remove this hurdle, we have developed a new R package `repro` that supports scholars in setting up, maintaining, and reproducing research projects in R.
In the remainder, we will walk you through a complete research project, from sketching the very first idea over preregistration, data analysis, and submission of a preprint while focusing on how the `repro`-package enables users to maintain reproducibility of all steps.


```{r schematic, eval = TRUE, echo = FALSE, fig.cap="Schematic illustration of the interplay of the four components (in dashed boxes) central to the reproducible workflow: version control (Git), dependency tracking (Make), software management (Docker), and dynamic document generation (R Markdown). Git tracks changes to the project over time. Make manages dependencies among the files. Docker provides a container in which the final report is built using dynamic document generation in R Markdown. Reproduced from @Peikert2019.  "}
# file gets downloaded in Makefile
knitr::include_graphics("images/nutshell.svg", auto_pdf = TRUE)
```

# Tutorial

A major hurdle for adoption of the before mentioned tools for researchers is that they are typically used through a command line interface (CLI).
The CLI is vastly more user friendly than its predecessor, the punch card, but many users (including the authors) prefer a graphical user interface (GUI) over text based interactions for most tasks.
A GUI has the obvious disadvantages for reproducibility, that it requires a user who clicks on the correct buttons in the correct order.
Depending on the number of steps this is not only much effort but also error prone.
This tutorial seeks a compromise.
We use a GUI (and similar helpers) where we can, to create a reproducible workflow but the reproduction of such workflow does not rely on manual steps.
At several points we rely on or have self developed a CLI that wraps a more complicated (and powerful) CLI.
These wrapper use standard CLI tools but provide feedback for the user about what the computer does and what the user should do in laymans terms.
We hope this make reproducibility tools more accessible by enabling untrained users to detect their systems state accurately and act correspondingly [@parasuramanAutomationHumanPerformance2018, Chapter 8: "Automation and Situation Awareness"].
These alternative tools are merely an assistance system and as users get more comfortable they can use the underlying tools directly to solve more complex problems.

One example for such assistance are the check functions from `repro`.




Two properties of CLI tools make them useful but at the same time difficult to learn.
Many CLI tools are decades old, and are therefore stable, widely used, and widely supported.
Because of their age they seem archaic and are difficult to use when compared to more modern computer tools.
These tools are also more powerful than many GUI counterparts, because of their flexibility---many options to change their behavior and composabibility---the output of one, can be the input of another.


# Preregistration as Code 

We argue that a preregistration as code (PAC) is an excellent planning tool that offers several benefits over traditional preregistration.
For a PAC, researchers write a reproducible, dynamically generated manuscript including the sections Introduction, Methods, and Results before they gather data.

We consider three criteria when planning a study.
First, the plan should be comprehensive, e.g. state the research question and describe the study design and analysis.
Second, the plan should be effective, meaning that the researchers can reasonably expect it to answer the research question at hand.
Third, the plan should be efficient; therefore, it only employs the necessary amount of resources.

To ensure a comprehensive plan, we suggest that researchers borrow from widely employed standards for writing an empirical manuscript.
The introduction and theoretical background provide the basis for the studies design and can be written before gathering data.
We argue that researchers can plan more effectively if they formulate the planned analysis as a method section that again follows best practices for describing empirical research.
To ensure that the analysis is technically feasible, they can translate their method section into executable computer code for the data analysis using simulated data.
The researchers can include a "faked " results section based on the simulated data, employing dynamic document generation.
Repeated simulation with varying parameters, a so-called Monte Carlo simulation, can then be employed to plan resources to assure efficiency.
For example, researchers can assess the power (the probability to detect an effect of a given size) for varying numbers of observations in the simulation to determine the required sample size for their study.

Defining the research questions and planning data analysis before observing the research outcomes is called preregistration [https://www.pnas.org/content/115/11/2600].
Preregistration increases the credibility of empirical results in three ways [preregistration is hard and worthwhile, Nosek].

> First, preregistration of analysis plans makes clear which analyses were planned a priori and which were conducted post hoc. This improves calibration of uncertainty for unplanned analyses, and diagnosticity of statistical inferences for planned analyses. [...] Doing so has the benefit of strengthening statistical inferences as compared with unplanned analyses. [...] Second, preregistration enables detection of questionable research practices such as selective outcome reporting (John et al. 2012) or Hypothesizing After the Results are Known (HARKing; Kerr, 1998). Third, preregistration of studies can reduce the impact of publication bias—particularly the prioritization of publishing positive over negative results—by making all planned studies discoverable whether or not they are ultimately published.

A preregistration as code offers four advantages over classical preregistration.
First, a PAC removes any ambiguity regarding the translation of an analysis plan into code.
Second, despite being rigorous, it offers the flexibility to incorporate data-dependent decisions if they can be formulated as code.
Researchers can, for example, formulate conditions under which they prefer one analysis over the other, i.e. if distributional assumptions are not met, employ robust methods or automated variable selection mechanisms.
Third, deviations from the preregistration are more explicit because they are reflected in changes to the codes a property facilitated by version control.
Fourth, the preregistration is merely a development stage of the final manuscript, thus saving the researchers from writing and the reviewer from evaluating two separate documents.

# References

```{r, include=FALSE}
knitr::write_bib(c(.packages(), "here", "rticles", "gert", "bookdown", "lavaan", "knitr", "targets", "renv", "tidyverse"),
                 "packages.bib")
```
